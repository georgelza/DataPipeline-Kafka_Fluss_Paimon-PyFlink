# Building IoT Pipeline, From source via Confluent Kakfa via Apache Flink (using UDF's) into Fluss.


## Overview

This originally started as a simple idea, create a IoT JSON packaged payload, publish it onto a **Confluent Kafka Broker**, consume using a connector into Flink and, do some fancy aggregation and then sink the aggregated numbers down into persisting layer.


As per the previous blog, this is the second set of labs. These labs follows the previous blog where simply consumed data from a Kafka topic, and pushed it into our Fluss environment and down into our Lakehouse storage based on Apache Paimon/Apache Parquet on Apache HDFS.

This time round, we're crawling a little deeper down our rabbit hole. Each of the below examples expands our knowledge a bit, bit by bit.

We will as per previous be publishing the IoT payloads generated by the 6 factories onto 3 seperate **Kafka Topics** based on the regional locatation/distribution, namely North, South and East.
      
For this 2nd set of examples we won't be using the lakehouse this time round, as we've shown that previously. We will however now play around with the data a bit using Apache Flink's User Defined Function (UDF's) based on Python capability. 

1.  Using a User Defined Function we will create a temporary table to consume the data from the Apache Kafka Topic, we then flatten the payload and and publish the new flat structure back onto a new `Apache/Confluent Kakfa:factory_iot_<region>_<siteId>` topic (UDF in `<root>/devlab1/pyFlink/flink_flat0.py`).
    
    Regions being:
    
    - North (sites 101 and 104)
    - South (sites 102 and 105) 
    - East (sites 103 and 106)

    You can use your Confluent Control center to view the data published on the new topic's.

2.  Next we do the same consumption, but instead of posting it back onto Apache Kafka, we now push it via Flink into an Fluss Table `fluss_fluss_catalog.fluss.factory_iot_unnested` (UDF in `<root>/devlab1/pyFlink/flink_flat1.py`).
 
    You can open Flink sql interface by executing `make fsql` and then copy/pasting the contents of `<root>/devlab0/pyFlink/query_flat.sql` to view the data being pushed into our Fluss table.

3.  Lastly we do the same consumption again, but this time use Apache flinks windowing capability (1 minute tumbles) to compute number of measurements received, min value read, avg value and max value. We then push (insert) these computed values grouped per siteId, deviceId, sensorId per window into a our Fluss backed table, `fluss_catalog.fluss.factory_iot_avg` (UDF in`<root>/devlab0/pyFlink/flink_avg1.py`).

    You can open Flink SQL interface by executing `make fsql` and then copy/pasting the contents of `<root>/devlab0/pyFlink/query_avg.sql` to view the data being pushed into our Fluss table.

NOTE: the above `<root>/devlab0/pyFlink/*` scripts are also accessible on the Flink Jobmanager under the `/pyapp` directory.


### Flink Catalog

We're still using our **Apache Hive Metastore** as catalog with a **PostgreSQL** database for backend storage.
See below for version information.


## Modules and Versions

- Confluent Kafka Cluster 7.9.1
  
- Apache Flink 1.20.1 - Java 17

- Apache Fluss 0.6.0

- Ubuntu 24.04 LTS

- Hive Metastore 3.1.3 on Hadoop 3.3.5 (OpenJDK8) on Ubuntu 24.04 LTS

- PostgreSQL 12

- Python 3.12


## Our various IoT Payloads formats.

### Basic min IoT Payload, produced by Factories 101 and 104

```json5
{
    "ts": 1729312551000, 
    "metadata": {
        "siteId": 101, 
        "deviceId": 1004, 
        "sensorId": 10034, 
        "unit": "BAR"
    }, 
    "measurement": 120
}
```

### Basic min IoT Payload, with a human readable time stamp & location added, produced by Factories 102 and 105

```json5
{
    "ts": 1713807946000, 
    "metadata": {
        "siteId": 102, 
        "deviceId": 1008, 
        "sensorId": 10073, 
        "unit": "Liter", 
        "ts_human": "2024-04-22T19:45:46.000000", 
        "location": {
            "latitude": -33.924869, 
            "longitude": 18.424055
        }
    }, 
    "measurement": 25
}
```

### Complete IoT Payload, with deviceType tag added, produced by Factories 103 and 106

```json5
{
    "ts": 1707882120000, 
    "metadata": {
        "siteId": 103, 
        "deviceId": 1014, 
        "sensorId": 10124, 
        "unit": "Amp", 
        "ts_human": "2024-02-14T05:42:00.000000", 
        "location": {
            "latitude": -33.9137, 
            "longitude": 25.5827
        }, 
        "deviceType": "Hoist_Motor"
    },
    "measurement": 24
}
```


## To run the project.

### See various configuration settings and passwords in:

0. `<root>/devlab0/docker_compose.yml`

1. `.pwd` in `<root>/app_iot#` and contents of `<root>/app_iot#/site#.sh`

2. `<root>/devlab0/.env`

3. `<root>/devlab0/conf/config.yaml`
   
4. `<root>/devlab0/conf/hive.env`

5. `<root>/devlab0/conf/hive-site.xml`


### Download containers and libraries

1. `cd <root>/infrastructure`

2. `make pullall`

3. `make buildall`


### Build various containers

1. `cd <root>/devlab0`

2. `./getlibs.sh`

3. `make build`

4. Now, to run it please read the `<root>/devlab0/README.md` file.


## Projects / Components

- [Apache Flink](https://flink.apache.org)

- [Ververica](https://www.ververica.com)

- [What is Fluss](https://alibaba.github.io/fluss-docs/)

- [Fluss Overview](https://alibaba.github.io/fluss-docs/docs/install-deploy/overview/)

- [What is Fluss docs](https://alibaba.github.io/fluss-docs/docs/intro/)

- [Fluss Project Git Repo](https://github.com/alibaba/fluss)

- [Introduction to Fluss](https://www.ververica.com/blog/introducing-fluss)

- [Apache Paimon](https://paimon.apache.org)

- [Apache Parquet File format](https://parquet.apache.org)


## Misc Notes

### Flink Libraries

As I am always travelling while writing these blog's and did not want to pull the libraries on every build I decided to downlaod them once into the below directory and then mount them into container. Just a different way less bandwidth and also slightly faster builds.

The `devlab#/conf/flink/lib/*` directories will house our Java libraries required by our Flink stack. 

Normally I'd include these in the Dockerfile as part of the image build, but during development it's easier if we place them here and then mount the directories into the containers at run time via our `docker-compose.yml` file inside the volume specification for the flink-* services.

This makes it simpler to add/remove libraries as we simply have to restart the flink container and not rebuild it.

Additionally, as the `jobmanager`, `taskmanager` use the same libraries doing it tis way allows us to use this one set, thus also reducing the disk space and the container image size.

The various files are downloaded by executing the `getlibs.sh` file located in the `devlab#/` directory.


### Flink base container images for 1.20.1 (manual pull from `hub.docker.com`)

- docker pull arm64v8/flink:1.20.1-scala_2.12-java11


### Self Build Flink container

- [Master Flink download](https://flink.apache.org/downloads/#apache-flink-1201)

- [Flink 1.20.1 binaries](https://www.apache.org/dyn/closer.lua/flink/flink-1.20.1/flink-1.20.1-bin-scala_2.12.tgz)


### Uncategorized notes and Articles

- [Apache Flink FLUSS](https://www.linkedin.com/posts/polyzos_fluss-is-now-open-source-activity-7268144336930832384-ds87?utm_source=share&utm_medium=member_desktop)

- [Apache Flink Deployment](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/deployment/resource-providers/standalone/docker/)    
    
- [Troubleshooting Apache Flink SQL S3 problems](https://www.decodable.co/blog/troubleshooting-flink-sql-s3-problems)


### Markdown syntax

The various `REAMDE.md` utilises markdown syntax. You can refer to `https://markdownlivepreview.com` & `https://dillinger.io` for more information, examples.

To view a mardown file, `https://jumpshare.com/viewer/md` or if you using Visual Studio Code search for markdown as a module and try some of them.


### HDFS Cluster

- [Setting Up an HDFS Cluster with Docker Compose: A Step-by-Step Guide](https://bytemedirk.medium.com/setting-up-an-hdfs-cluster-with-docker-compose-a-step-by-step-guide-4541cd15b168)
- [Deploying Hadoop using Docker](https://medium.com/@garin.sanny07/hadoop-cluster-55477505d0ff)

- [Installing the Hadoop Stack using Docker](https://hackmd.io/@silicoflare/docker-hadoop)


### Flink Cluster

- [how-to-set-up-a-local-flink-cluster-using-docker](https://medium.com/marionete/how-to-set-up-a-local-flink-cluster-using-docker-0a0a741504f6)


### RocksDB

- [Using RocksDB State Backend in Apache Flink: When and How](https://flink.apache.org/2021/01/18/using-rocksdb-state-backend-in-apache-flink-when-and-how/)


### Log4J Logging levels

- [Log4J Logging Levels](https://logging.apache.org/log4j/2.x/manual/customloglevels.html)
    
- The Flink jobmanager and taskmanager log levels can be modified by editing the various `devlab#/conf/*.properties` files. Remember to restart your Flink containers.


### Great quick reference for docker compose

- [A Deep dive into Docker Compose by Alex Merced](https://dev.to/alexmercedcoder/a-deep-dive-into-docker-compose-27h5)


### Consider using secrets for sensitive information

- [How to use sectrets with Docker Compose](https://docs.docker.com/compose/how-tos/use-secrets/)


### Credits:

This blog would not have been possible without the assistance of people, 2 that I do need to mention by name being Jark Wu (the product owner of Fluss at Alibaba) and Dian Fu (PyFlink owner at Albaba)

    Jark Wu
        Head of Fluss and Flink SQL at Alibaba Cloud (Ververica) | PMC member and Committer of Apache Flink
        https://www.linkedin.com/in/jarkwu/

    Dian Fu
        Senior Technical Specialist at Alibaba Cloud
        https://www.linkedin.com/in/dian-fu-07797493/


### By:

George

[georgelza@gmail.com](georgelza@gmail.com)

[George on Linkedin](https://www.linkedin.com/in/george-leonard-945b502/)

[George on Medium](https://medium.com/@georgelza)

